# Presentation of the project

We are doing a DevOps project, assisted by two employees from takima.
Create an appropriate file structure, 1 folder per image.

Target application
3-tiers application:
- HTTP server
- Backend API
- Database

For each of those applications, we will follow the same process: choose the appropriate docker base image, create and configure this image, put our application specifics inside and at some point have it running. Our final goal is to have a 3-tier web API running.
This have to be CI pipeline which also stocking the images of the project.

http://school.pages.takima.io/devops-resources/

Part 1 -[Docker sessions](http://school.pages.takima.io/devops-resources/ch1-discover-docker-tp/)
Part 2 - [GitHub Action session](http://school.pages.takima.io/devops-resources/ch2-discover-github-actions-tp/)
Part 3 - [Ansible session](http://school.pages.takima.io/devops-resources/ch3-discover-ansible-tp/)

## TECHNOLOGIES

- Docker Engine (images, containers, volume, network, Dockerfile, docker-compose)
- PostgreSQL
- Adminer
- .sql scripts
- Java (Maven, jdk-17)
- HTML
- GitHub
- Docker Hub
- GitHub Actions
- yaml files
- SonarCloud
- Ansible


### ACCOUNTS

**GitHub**
username: oscar92i
https://github.com/oscar92i/takima

**Docker HUB**
username: oscarepf
https://hub.docker.com/repositories/oscarepf



# Code ressources

### Installation of Docker Engine on Ubuntu:

``` 
#Add Docker's official GPG key:
sudo apt-get update
sudo apt-get install ca-certificates curl
sudo install -m 0755 -d /etc/apt/keyrings
sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
sudo chmod a+r /etc/apt/keyrings/docker.asc

#Add the repository to Apt sources:
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt-get update

sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

sudo docker run hello-world
``` 
## TP part 1

We pull adminer images
```
docker pull adminer
```
We create a docker network named app-network. Docker networks are a fundamental aspect of container orchestration and management. They provide the necessary infrastructure for container communication, security, and scalability. Whether you are running a simple application on a single host or a complex microservices architecture across multiple hosts, Docker networks offer the flexibility and control needed to build robust and efficient containerized applications.
```
docker network create app-network
```
Now let's create our database container from the following Dockerfile.
```
FROM postgres:14.1-alpine

ENV POSTGRES_DB=db \
   POSTGRES_USER=usr \
   POSTGRES_PASSWORD=pwd
```
```
sudo docker run -d --name my_postgrey_app --network app-network -p 5432:5432 oscar/tp1
sudo docker run -d --name adminer --network app-network -p 8090:8080 adminer
```
We add to the Dockerfile
``` 
COPY ./sql-scripts/* /docker-entrypoint-initdb.d
```
then we rebuild and re run the database image.
```
sudo docker rm -f my_postgrey_app
sudo docker build . -t oscar/tp1
sudo docker run -d --name my_postgrey_app --network app-network -p 5432:5432 oscar/tp1
```
Volumes are the preferred mechanism for persisting data generated by and used by Docker containers (like databases).
-v /my/own/datadir:/var/lib/postgresql/data
```
sudo docker run -d --name my-postgrey-app --network app-network -v /home/omigeon/devops/tpdocker/database:/var/lib/my-postgrey-app/data -p 5432:5432 my-postgres-db
```
```
docker run -d \
  -p "8090:8080" \
  --net=app-network \
  --name=adminer \
  adminer
```
Let's inspect.
```
sudo docker volume inspect volume-tp1
```
Output:
[
    {
        "CreatedAt": "2024-05-28T10:55:07+02:00",
        "Driver": "local",
        "Labels": null,
        "Mountpoint": "/var/lib/docker/volumes/volume-tp1/_data",
        "Name": "volume-tp1",
        "Options": null,
        "Scope": "local"
    }
]

### Multistage

We have to create the GreetingController class. 
path : /home/omigeon/devops/tpdocker/backend-api/simpleapi/src/main/java/fr/takima/training/simpleapi/Controller

How to integrate multistage ?
Uses a Maven image to compile the Java application.
Copies necessary files (pom.xml, src) and runs the mvn package command to build the JAR file.
Run Stage:
Uses a lighter Amazon Corretto image for running the application, reducing the final image size.
Copies the compiled JAR file from the build stage.
Sets the entry point to run the JAR file with Java.

Hy should we use multistage ?
This approach ensures a clean, minimal runtime environment with only the necessary dependencies to run the application, leading to smaller, more secure, and efficient Docker images.

### httpd
```
sudo docker build -t frontend_app .
sudo docker run -d --name frontend_app -p 8082:80 --network app-network frontend_app
```
Then open your web browser on localhost:8082 and we see what is written in the index.html

We end up by creating a docker-compose.yml in the src with the three container.
Docker Compose is crucial for modern application development due to its ability to manage multi-container applications efficiently, ensuring consistency, simplifying networking and dependencies, providing easy volume management, supporting multi-environment setups, allowing easy scaling, and offering a simplified command interface. This makes it a powerful tool for both development and production environments.
```
sudo docker compose up -d
```
Output:
[+] Running 3/3
 ✔ Container my_postgrey_app  Started                                                                                             3.3s 
 ✔ Container backend-api      Started                                                                                             3.6s 
 ✔ Container frontend_app     Started 

Let's connect to Docker Hub
```
sudo docker login --username oscarepf --password *********
```
WARNING! Using --password via the CLI is insecure. Use --password-stdin.
WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

Login Succeeded

We can push our images. One example below.
```
sudo docker tag tpdocker-frontend oscarepf/tpdocker-frontend:1.0
sudo docker push oscarepf/tpdocker-frontend:1.0
```
## TP Part02

Testcontainers:
Testcontainers is a popular library that provides lightweight, disposable instances of common databases, Selenium web browsers, and other services running in Docker containers. It's widely used for integration testing in Java applications. By using Testcontainers, developers can ensure that their tests run in environments that closely resemble production, improving the reliability and reproducibility of tests

Do not forget to use GitHub for code versioning.
```
git init
git add *all your docs you want to push*
git commit -m "first commit"
git branch -M main
git remote add origin https://github.com/oscar92i/takima.git
git push -u origin main
```

For what purpose do we need to push docker images?
Pushing Docker images is essential for ensuring consistent, portable, scalable, and manageable deployments across different environments. It plays a critical role in modern software development and operations, enabling efficient CI/CD practices, microservices architectures, and cloud-native applications.


Repository secrets

SonarCloud

Project_key: takima-oscar_takima-devops
Organization_key: takima-oscar

      # Build and test with Maven
      - name: Build and test with Maven
        run: mvn -B verify sonar:sonar -Dsonar.projectKey=takima-oscar_takima-devops -Dsonar.organization=takima-oscar -Dsonar.host.url=https://sonarcloud.io -Dsonar.login=${{ secrets.SONAR_TOKEN }}  --file ./backend-api/simple-api-student
### TP part 3

ANSIBLE
```
sudo apt install pipx
pipx install --include-deps ansible
pipx install ansible-core
```
SSH

A this point we need to connect using ssh protocol.
This can't be done through the EPF VM because it is connected to the EPF WiFi.

So I tried to stop using the EPF VM, but due to the fact that I use an EPF computer, and maybe that I am not an administrator, I couldn't download WSL.

I am not able to end the part 3.